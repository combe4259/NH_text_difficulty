"""
í…ìŠ¤íŠ¸ ë‚œì´ë„ ìë™ ë¼ë²¨ë§ í”„ë¡œê·¸ë¨
Google Colabì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.

ì‚¬ìš©ë²•:
1. Google Colabì—ì„œ ì´ íŒŒì¼ ì—…ë¡œë“œ
2. GPU ëŸ°íƒ€ì„ ì„¤ì •
3. ì‹¤í–‰: !python text_difficulty_labeler.py
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
import pandas as pd
from tqdm import tqdm
import re
import time
import os
import json
import matplotlib.pyplot as plt
from datetime import datetime

# PDF ì²˜ë¦¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import pdfplumber
    PDF_SUPPORT = True
    # ê°œì„ ëœ ì¶”ì¶œê¸°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    try:
        from improved_pdf_extractor import ImprovedPDFExtractor
        IMPROVED_EXTRACTOR = True
    except ImportError:
        IMPROVED_EXTRACTOR = False
except ImportError:
    PDF_SUPPORT = False
    IMPROVED_EXTRACTOR = False
    print("âš ï¸ PDF ì§€ì›ì„ ìœ„í•´ ì„¤ì¹˜ í•„ìš”: pip install pdfplumber")

class TextDifficultyLabeler:
    def __init__(self, model_name="google/gemma-2-2b-it", hf_token=None):
        """
        í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¼ë²¨ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            model_name: HuggingFace ëª¨ë¸ëª…
            hf_token: HuggingFace í† í° (gated ëª¨ë¸ìš©)
        """
        self.model_name = model_name

        # HuggingFace ë¡œê·¸ì¸ (í•„ìš”ì‹œ)
        if hf_token:
            login(token=hf_token)
            print("âœ… HuggingFace ë¡œê·¸ì¸ ì™„ë£Œ")

        # ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.load_model()
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

        # ê²°ê³¼ ì €ì¥ìš©
        self.results = []

    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            # 4bit ì–‘ìí™” ì‹œë„
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                load_in_4bit=True,
                device_map="auto",
                torch_dtype=torch.float16,
            )
        except:
            # 4bit ì‹¤íŒ¨ì‹œ ì¼ë°˜ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map="auto",
                torch_dtype=torch.float16,
            )

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def create_prompt(self, text):
        """ë‚œì´ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë‚œì´ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text[:300]}"

í‰ê°€ ê¸°ì¤€:
1 = ì´ˆë“±í•™êµ ì €í•™ë…„ (ë§¤ìš° ì‰¬ì›€) - ê¸°ë³¸ ì–´íœ˜, ì§§ì€ ë¬¸ì¥
2 = ì´ˆë“±í•™êµ ê³ í•™ë…„ (ì‰¬ì›€) - ì¼ìƒ ì–´íœ˜, ê°„ë‹¨í•œ ë¬¸ì¥
3 = ì¤‘í•™ìƒ (ë³´í†µ) - êµê³¼ì„œ ìˆ˜ì¤€, ë³µí•©ë¬¸
4 = ê³ ë“±í•™ìƒ (ì–´ë ¤ì›€) - ì „ë¬¸ìš©ì–´ ì¼ë¶€, ë³µì¡í•œ êµ¬ì¡°
5 = ëŒ€í•™ìƒ/ì „ë¬¸ê°€ (ë§¤ìš° ì–´ë ¤ì›€) - ì „ë¬¸ìš©ì–´ ë‹¤ìˆ˜, ë‚œí•´í•œ ê°œë…

ì˜ˆì‹œ:
- "ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤." â†’ 1
- "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ ê³µì›ì— ê°”ì–´ìš”." â†’ 2
- "ë¯¼ì£¼ì£¼ì˜ëŠ” êµ­ë¯¼ì´ ì£¼ê¶Œì„ ê°€ì§„ ì •ì¹˜ì²´ì œì…ë‹ˆë‹¤." â†’ 3
- "ì–‘ìì—­í•™ì˜ ë¶ˆí™•ì •ì„± ì›ë¦¬ì— ë”°ë¥´ë©´..." â†’ 5

ìˆ«ìë§Œ ë‹µí•˜ì„¸ìš”.
ë‚œì´ë„:"""

        return prompt

    def get_difficulty(self, text):
        """í…ìŠ¤íŠ¸ ë‚œì´ë„ í‰ê°€"""
        prompt = self.create_prompt(text)

        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )

        # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
        if torch.cuda.is_available():
            inputs = inputs.to('cuda')

        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=10,
                temperature=0.3,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # ë””ì½”ë”©
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[-1]:],
            skip_special_tokens=True
        )

        # ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r'[1-5]', response)
        if numbers:
            return int(numbers[0])

        # ê¸°ë³¸ê°’
        return 3

    def label_texts(self, texts, batch_save=10, checkpoint_path=None):
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë¼ë²¨ë§
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_save: Nê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        """
        # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        processed_texts = set()
        if checkpoint_path and os.path.exists(checkpoint_path):
            checkpoint_df = pd.read_csv(checkpoint_path)
            processed_texts = set(checkpoint_df['text'].tolist())
            self.results = checkpoint_df.to_dict('records')
            print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {len(processed_texts)}ê°œ ì´ë¯¸ ì²˜ë¦¬ë¨")

        # ë¼ë²¨ë§ ì‹œì‘
        new_results = []

        for i, text in enumerate(tqdm(texts, desc="ë¼ë²¨ë§ ì§„í–‰")):
            # ì´ë¯¸ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ëŠ” ìŠ¤í‚µ
            if text in processed_texts:
                continue

            try:
                # ë‚œì´ë„ í‰ê°€
                difficulty = self.get_difficulty(text)

                result = {
                    'text': text,
                    'difficulty': difficulty,
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }

                new_results.append(result)
                self.results.append(result)

                # ë°°ì¹˜ ì €ì¥
                if checkpoint_path and len(new_results) % batch_save == 0:
                    self.save_checkpoint(new_results, checkpoint_path)
                    new_results = []

                # ì†ë„ ì¡°ì ˆ
                time.sleep(0.1)

            except Exception as e:
                print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
                print(f"   ë¬¸ì œ í…ìŠ¤íŠ¸: {text[:50]}...")
                continue

        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥
        if checkpoint_path and new_results:
            self.save_checkpoint(new_results, checkpoint_path)

        print(f"âœ… ë¼ë²¨ë§ ì™„ë£Œ: ì´ {len(self.results)}ê°œ")

        return pd.DataFrame(self.results)

    def save_checkpoint(self, new_results, checkpoint_path):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        df = pd.DataFrame(new_results)
        if os.path.exists(checkpoint_path):
            df.to_csv(checkpoint_path, mode='a', header=False, index=False)
        else:
            df.to_csv(checkpoint_path, index=False)
        print(f"  ğŸ’¾ {len(new_results)}ê°œ ì €ì¥ë¨")

    def save_results(self, output_dir='/content/drive/MyDrive'):
        """ê²°ê³¼ ì €ì¥ (CSV, Excel, JSON)"""
        df = pd.DataFrame(self.results)

        if df.empty:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # íƒ€ì„ìŠ¤íƒ¬í”„
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # CSV ì €ì¥
        csv_path = os.path.join(output_dir, f'labeled_data_{timestamp}.csv')
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… CSV ì €ì¥: {csv_path}")

        # Excel ì €ì¥
        excel_path = os.path.join(output_dir, f'labeled_data_{timestamp}.xlsx')
        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"âœ… Excel ì €ì¥: {excel_path}")

        # JSON ì €ì¥ (Fine-tuningìš©)
        json_data = []
        for _, row in df.iterrows():
            json_data.append({
                "text": row['text'],
                "label": int(row['difficulty']) - 1,  # 0-4ë¡œ ë³€í™˜
                "difficulty": int(row['difficulty'])  # ì›ë³¸ ìœ ì§€
            })

        json_path = os.path.join(output_dir, f'training_data_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSON ì €ì¥: {json_path}")

        return csv_path, excel_path, json_path

    def visualize_results(self, save_path=None):
        """ê²°ê³¼ ì‹œê°í™”"""
        df = pd.DataFrame(self.results)

        if df.empty:
            print("âš ï¸ ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ê·¸ë˜í”„ ìƒì„±
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))

        # 1. ë‚œì´ë„ ë¶„í¬
        difficulty_counts = df['difficulty'].value_counts().sort_index()
        axes[0].bar(difficulty_counts.index, difficulty_counts.values, color='steelblue')
        axes[0].set_xlabel('ë‚œì´ë„')
        axes[0].set_ylabel('í…ìŠ¤íŠ¸ ê°œìˆ˜')
        axes[0].set_title('ë‚œì´ë„ ë¶„í¬')
        axes[0].set_xticks(range(1, 6))
        axes[0].grid(axis='y', alpha=0.3)

        # 2. ë‚œì´ë„ë³„ ë¹„ìœ¨
        colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c', '#9b59b6']
        axes[1].pie(difficulty_counts.values,
                    labels=[f'Level {i}' for i in difficulty_counts.index],
                    colors=colors[:len(difficulty_counts)],
                    autopct='%1.1f%%')
        axes[1].set_title('ë‚œì´ë„ë³„ ë¹„ìœ¨')

        plt.suptitle(f'í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„ ê²°ê³¼ (ì´ {len(df)}ê°œ)', fontsize=16)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=100, bbox_inches='tight')
            print(f"âœ… ê·¸ë˜í”„ ì €ì¥: {save_path}")

        plt.show()

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        df = pd.DataFrame(self.results)

        if df.empty:
            print("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print("\n" + "="*50)
        print("ğŸ“Š ë¼ë²¨ë§ ê²°ê³¼ ìš”ì•½")
        print("="*50)

        print(f"ì´ í…ìŠ¤íŠ¸ ìˆ˜: {len(df)}ê°œ")
        print(f"\në‚œì´ë„ ë¶„í¬:")

        difficulty_names = {
            1: "ë§¤ìš° ì‰¬ì›€ (ì´ˆë“± ì €í•™ë…„)",
            2: "ì‰¬ì›€ (ì´ˆë“± ê³ í•™ë…„)",
            3: "ë³´í†µ (ì¤‘í•™ìƒ)",
            4: "ì–´ë ¤ì›€ (ê³ ë“±í•™ìƒ)",
            5: "ë§¤ìš° ì–´ë ¤ì›€ (ëŒ€í•™/ì „ë¬¸ê°€)"
        }

        for difficulty in range(1, 6):
            count = len(df[df['difficulty'] == difficulty])
            percentage = (count / len(df)) * 100 if len(df) > 0 else 0
            print(f"  Level {difficulty} - {difficulty_names[difficulty]}: {count}ê°œ ({percentage:.1f}%)")

        print(f"\ní‰ê·  ë‚œì´ë„: {df['difficulty'].mean():.2f}")
        print(f"ì¤‘ì•™ê°’: {df['difficulty'].median():.1f}")

        # ìƒ˜í”Œ ì¶œë ¥
        print("\nğŸ“ ìƒ˜í”Œ í…ìŠ¤íŠ¸:")
        for difficulty in range(1, 6):
            samples = df[df['difficulty'] == difficulty].head(1)
            if not samples.empty:
                text = samples.iloc[0]['text']
                print(f"\nLevel {difficulty}: {text[:80]}...")


# ============ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ============

def extract_texts_from_pdf(pdf_path, split_mode='smart', use_improved=True):
    """
    PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸ˆìœµ/ë²•ë¥  ë¬¸ì„œì— ìµœì í™”)
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        split_mode: í…ìŠ¤íŠ¸ ë¶„ë¦¬ ë°©ì‹
            - 'smart': ì§€ëŠ¥í˜• ë¶„ë¦¬ (ë²ˆí˜¸ í•­ëª© + ë¬¸ì¥ ë³µí•©)
            - 'sentence': ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
            - 'paragraph': ë‹¨ë½ ë‹¨ìœ„ ë¶„ë¦¬
            - 'bullet': ë²ˆí˜¸/ê¸°í˜¸ í•­ëª© ë‹¨ìœ„
            - 'page': í˜ì´ì§€ ë‹¨ìœ„
        use_improved: ê°œì„ ëœ ì¶”ì¶œê¸° ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if not PDF_SUPPORT:
        print("âŒ pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì‹¤í–‰: pip install pdfplumber")
        return []
    
    # ê°œì„ ëœ ì¶”ì¶œê¸°ê°€ ìˆê³  ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ëœ ê²½ìš°
    if use_improved and IMPROVED_EXTRACTOR and split_mode == 'smart':
        print("ğŸš€ ê°œì„ ëœ PDF ì¶”ì¶œê¸° ì‚¬ìš©")
        extractor = ImprovedPDFExtractor(pdf_path)
        return extractor.extract_all(mode='smart')

    texts = []

    try:
        with pdfplumber.open(pdf_path) as pdf:
            print(f"ğŸ“„ PDF íŒŒì¼ ì—´ê¸°: {pdf_path}")
            print(f"   ì´ {len(pdf.pages)}í˜ì´ì§€")
            print(f"   ë¶„ë¦¬ ëª¨ë“œ: {split_mode}")

            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            all_text = ""
            for page_num, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text()
                if page_text:
                    all_text += page_text + "\n"

            if split_mode == 'smart':
                # ì§€ëŠ¥í˜• ë¶„ë¦¬: ê¸ˆìœµ/ë²•ë¥  ë¬¸ì„œì— ìµœì í™”
                texts = extract_smart_segments(all_text)
                
            elif split_mode == 'sentence':
                # ê¸°ë³¸ ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
                sentences = re.split(r'[.!?]+', all_text)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 10:
                        texts.append(sentence)
                        
            elif split_mode == 'paragraph':
                # ë‹¨ë½ ë‹¨ìœ„ ë¶„ë¦¬ (ì¤„ë°”ê¿ˆ 2ê°œ ì´ìƒ)
                paragraphs = re.split(r'\n\n+', all_text)
                for para in paragraphs:
                    para = para.strip()
                    if len(para) > 20:
                        texts.append(para)
                        
            elif split_mode == 'bullet':
                # ë²ˆí˜¸/ê¸°í˜¸ í•­ëª© ë‹¨ìœ„ ë¶„ë¦¬
                texts = extract_bullet_items(all_text)
                
            else:  # 'page' ë˜ëŠ” ê¸°íƒ€
                # í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì €ì¥
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text and len(page_text.strip()) > 10:
                        texts.append(page_text.strip())

        print(f"âœ… {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
        
        # ì²˜ìŒ 3ê°œ ìƒ˜í”Œ ì¶œë ¥
        if texts:
            print("\nğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
            for i, text in enumerate(texts[:3], 1):
                preview = text[:60] + "..." if len(text) > 60 else text
                print(f"   {i}. {preview}")
        
        return texts

    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return []


def extract_smart_segments(text):
    """
    ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„ë¦¬ (ê¸ˆìœµ/ë²•ë¥  ë¬¸ì„œ ìµœì í™”)
    ë²ˆí˜¸ í•­ëª©, ì¡°ê±´ì ˆ, ì„œë¸Œì„¹ì…˜ ë“±ì„ ê°œë³„ì ìœ¼ë¡œ ë¶„ë¦¬
    """
    segments = []
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í…Œì´ë¸” êµ¬ì¡° ì •ë¦¬
    text = preprocess_text_for_extraction(text)
    
    # 1ë‹¨ê³„: ì£¼ìš” ì„¹ì…˜ ë¶„ë¦¬ (ëŒ€ì œëª© ê¸°ì¤€)
    section_patterns = [
        r'^ì œ\s*\d+\s*[ì¡°í•­ê´€]',      # ì œ1ì¡°, ì œ2í•­ ë“±
        r'^\d+\s*\.\s*[ê°€-í£]+',      # 1. ì œëª©
        r'^[A-Z]\.\s*',               # A. B. C.
        r'^[â… â…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©]\.',     # ë¡œë§ˆ ìˆ«ì
    ]
    
    # 2ë‹¨ê³„: ë²ˆí˜¸ í•­ëª© ë¶„ë¦¬ (ë” ì •í™•í•œ íŒ¨í„´)
    bullet_patterns = [
        r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',        # ì› ë²ˆí˜¸
        r'^[â¶â·â¸â¹âºâ»â¼â½â¾â¿]',        # ê²€ì€ ì› ë²ˆí˜¸  
        r'^[â‘´â‘µâ‘¶â‘·â‘¸â‘¹â‘ºâ‘»â‘¼â‘½]',        # ê´„í˜¸ ë²ˆí˜¸
        r'^\d+\)',                    # 1) 2) 3)
        r'^[ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨]\)',  # ê°€) ë‚˜) ë‹¤)
        r'^[-â€¢â–ªâ–«â—¦â€»]',                # ë¶ˆë¦¿ í¬ì¸íŠ¸
        r'^\*',                       # ë³„í‘œ
    ]
    
    # ëª¨ë“  íŒ¨í„´ í†µí•©
    all_patterns = '|'.join(section_patterns + bullet_patterns)
    
    # í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    lines = text.split('\n')
    current_segment = []
    current_type = None
    
    for i, line in enumerate(lines):
        original_line = line
        line = line.strip()
        
        if not line:
            # ë¹ˆ ì¤„ì´ ë‚˜ì˜¤ë©´ í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì¢…ë£Œ
            if current_segment and len(' '.join(current_segment).strip()) > 10:
                segments.append(' '.join(current_segment).strip())
                current_segment = []
                current_type = None
            continue
        
        # í…Œì´ë¸” í—¤ë” ê°ì§€ (ì„œë¹„ìŠ¤êµ¬ë¶„, ìš°ëŒ€ë‚´ìš© ë“±)
        if is_table_header(line):
            # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
            if current_segment:
                segments.append(' '.join(current_segment).strip())
                current_segment = []
            # í…Œì´ë¸” ì²˜ë¦¬
            table_segments = extract_table_segments(lines[i:])
            segments.extend(table_segments)
            # í…Œì´ë¸” ë¶€ë¶„ ìŠ¤í‚µ
            skip_lines = count_table_lines(lines[i:])
            for _ in range(skip_lines - 1):
                if i < len(lines) - 1:
                    i += 1
            continue
            
        # íŒ¨í„´ ë§¤ì¹­ í™•ì¸
        pattern_match = re.match(all_patterns, line)
        
        if pattern_match:
            # ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
            if current_segment:
                segment_text = ' '.join(current_segment).strip()
                if len(segment_text) > 10:
                    segments.append(segment_text)
            # ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘
            current_segment = [line]
            current_type = detect_segment_type(line)
        else:
            # ë“¤ì—¬ì“°ê¸°ë‚˜ ì—°ì†ëœ ë‚´ìš©ì¸ ê²½ìš°
            if current_segment:
                # ê°™ì€ ì„¸ê·¸ë¨¼íŠ¸ì— ì¶”ê°€
                current_segment.append(line)
            else:
                # ë…ë¦½ì ì¸ í…ìŠ¤íŠ¸
                if len(line) > 10:
                    segments.append(line)
    
    # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
    if current_segment:
        segment_text = ' '.join(current_segment).strip()
        if len(segment_text) > 10:
            segments.append(segment_text)
    
    # í›„ì²˜ë¦¬: ë„ˆë¬´ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë‹¤ì‹œ ë¶„ë¦¬
    final_segments = []
    for seg in segments:
        if len(seg) > 500:  # 500ì ì´ìƒì´ë©´
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‹¤ì‹œ ë¶„ë¦¬
            sub_sentences = re.split(r'(?<=[.!?])\s+', seg)
            for sub in sub_sentences:
                if len(sub.strip()) > 10:
                    final_segments.append(sub.strip())
        else:
            final_segments.append(seg)
    
    # ì¤‘ë³µ ì œê±°
    unique_segments = []
    seen = set()
    
    for seg in final_segments:
        normalized = ' '.join(seg.split())
        if normalized not in seen and len(normalized) > 10:
            seen.add(normalized)
            unique_segments.append(normalized)
    
    return unique_segments


def preprocess_text_for_extraction(text):
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í…Œì´ë¸”ì´ë‚˜ íŠ¹ìˆ˜ êµ¬ì¡° ì •ë¦¬
    """
    # ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
    text = re.sub(r'[ \t]+', ' ', text)
    
    # í…Œì´ë¸” êµ¬ë¶„ì ì²˜ë¦¬
    text = re.sub(r'(\S)\s{3,}(\S)', r'\1 | \2', text)  # 3ê°œ ì´ìƒ ê³µë°±ì€ êµ¬ë¶„ìë¡œ
    
    return text


def is_table_header(line):
    """
    í…Œì´ë¸” í—¤ë”ì¸ì§€ í™•ì¸
    """
    table_headers = [
        'ì„œë¹„ìŠ¤êµ¬ë¶„', 'ìš°ëŒ€ë‚´ìš©', 'ìš°ëŒ€ì¡°ê±´', 'ì ìš©ê¸°ì¤€',
        'êµ¬ë¶„', 'ë‚´ìš©', 'ì¡°ê±´', 'ë¹„ê³ ', 'í•­ëª©', 'ì„¤ëª…'
    ]
    
    for header in table_headers:
        if header in line and len(line.split()) <= 5:
            return True
    return False


def extract_table_segments(lines):
    """
    í…Œì´ë¸” í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì¶”ì¶œ
    """
    segments = []
    current_row = []
    
    for line in lines:
        line = line.strip()
        
        # í…Œì´ë¸” ë ê°ì§€
        if not line or re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤]', line):
            if current_row:
                segments.append(' '.join(current_row))
            break
            
        # ë²ˆí˜¸ í•­ëª©ì´ ìˆëŠ” í–‰
        if re.match(r'^[â¶â·â¸â¹âºâ»â¼â½â¾â¿]', line):
            if current_row:
                segments.append(' '.join(current_row))
            current_row = [line]
        else:
            if current_row:
                current_row.append(line)
    
    if current_row:
        segments.append(' '.join(current_row))
    
    return [s for s in segments if len(s) > 10]


def count_table_lines(lines):
    """
    í…Œì´ë¸”ì´ ì°¨ì§€í•˜ëŠ” ì¤„ ìˆ˜ ê³„ì‚°
    """
    count = 0
    for line in lines:
        line = line.strip()
        if not line or re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤]', line):
            break
        count += 1
    return count


def detect_segment_type(line):
    """
    ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì… ê°ì§€
    """
    if re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]', line):
        return 'main_item'
    elif re.match(r'^[â¶â·â¸â¹âºâ»â¼â½â¾â¿]', line):
        return 'sub_item'
    elif re.match(r'^ì œ\s*\d+\s*[ì¡°í•­ê´€]', line):
        return 'article'
    elif re.match(r'^\d+\)', line):
        return 'numbered'
    else:
        return 'other'


def extract_bullet_items(text):
    """
    ë²ˆí˜¸/ê¸°í˜¸ í•­ëª©ë§Œ ì¶”ì¶œ
    """
    items = []
    
    # ë²ˆí˜¸/ê¸°í˜¸ íŒ¨í„´
    patterns = [
        (r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]', 'ì›ë²ˆí˜¸'),
        (r'[â¶â·â¸â¹âºâ»â¼â½â¾â¿]', 'ê²€ì€ì›'),
        (r'\d+\)', 'ìˆ«ìê´„í˜¸'),
        (r'[ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨]\)', 'í•œê¸€ê´„í˜¸'),
        (r'[-â€¢â–ªâ–«â—¦]', 'ë¶ˆë¦¿'),
    ]
    
    lines = text.split('\n')
    current_item = []
    current_type = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # íŒ¨í„´ ë§¤ì¹­
        matched = False
        for pattern, ptype in patterns:
            if re.match(f'^{pattern}', line):
                # ì´ì „ í•­ëª© ì €ì¥
                if current_item:
                    item_text = ' '.join(current_item).strip()
                    if len(item_text) > 10:
                        items.append(item_text)
                
                # ìƒˆ í•­ëª© ì‹œì‘
                current_item = [line]
                current_type = ptype
                matched = True
                break
        
        if not matched and current_item:
            # í˜„ì¬ í•­ëª©ì— ê³„ì† ì¶”ê°€
            current_item.append(line)
    
    # ë§ˆì§€ë§‰ í•­ëª© ì €ì¥
    if current_item:
        item_text = ' '.join(current_item).strip()
        if len(item_text) > 10:
            items.append(item_text)
    
    return items


def extract_texts_from_multiple_pdfs(pdf_paths, split_mode='smart'):
    """
    ì—¬ëŸ¬ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    
    Args:
        pdf_paths: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        split_mode: í…ìŠ¤íŠ¸ ë¶„ë¦¬ ë°©ì‹ (smart/sentence/paragraph/bullet/page)
    
    Returns:
        ëª¨ë“  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    all_texts = []

    for pdf_path in pdf_paths:
        texts = extract_texts_from_pdf(pdf_path, split_mode)
        all_texts.extend(texts)

    print(f"\nğŸ“Š ì „ì²´ ì¶”ì¶œ ê²°ê³¼:")
    print(f"   â€¢ PDF íŒŒì¼ ìˆ˜: {len(pdf_paths)}ê°œ")
    print(f"   â€¢ ì´ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸: {len(all_texts)}ê°œ")
    
    return all_texts


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # Colab í™˜ê²½ í™•ì¸
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        print("âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ")
        is_colab = True
    except:
        print("âš ï¸ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...")
        is_colab = False

    # PDF ì§€ì› í™•ì¸
    if not PDF_SUPPORT:
        print("\nğŸ“Œ PDF ì²˜ë¦¬ë¥¼ ìœ„í•´ ì„¤ì¹˜:")
        print("   !pip install pdfplumber")

    # ì„¤ì •
    HF_TOKEN = None  # í™˜ê²½ ë³€ìˆ˜ë‚˜ ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ì„¸ìš”
    MODEL_NAME = "google/gemma-2-2b-it"  # ë˜ëŠ” "Qwen/Qwen2.5-1.5B-Instruct"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if is_colab:
        OUTPUT_DIR = "/content/drive/MyDrive/text_difficulty_labels"
    else:
        OUTPUT_DIR = "./labeled_data"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ë¼ë²¨ëŸ¬ ì´ˆê¸°í™”
    labeler = TextDifficultyLabeler(
        model_name=MODEL_NAME,
        hf_token=HF_TOKEN  # Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ ì°¾ìŒ
    )

    # ===== í…ìŠ¤íŠ¸ ì¤€ë¹„ =====
    
    # split_mode ì˜µì…˜:
    # - 'smart': ì§€ëŠ¥í˜• ë¶„ë¦¬ (ê¸ˆìœµ/ë²•ë¥  ë¬¸ì„œ ì¶”ì²œ) â­
    # - 'sentence': ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
    # - 'paragraph': ë‹¨ë½ ë‹¨ìœ„ ë¶„ë¦¬  
    # - 'bullet': ë²ˆí˜¸/ê¸°í˜¸ í•­ëª© ë‹¨ìœ„
    # - 'page': í˜ì´ì§€ ë‹¨ìœ„

    #ì˜µì…˜ 1: ë‹¨ì¼ PDF (smart ëª¨ë“œ ì‚¬ìš©)
    pdf_path = "/content/drive/MyDrive/10000831_pi.pdf"
    texts = extract_texts_from_pdf(pdf_path, split_mode='smart')  # 'smart' ëª¨ë“œë¡œ ë³€ê²½!

    #ì˜µì…˜ 2: ì—¬ëŸ¬ PDF
    #pdf_files = [
    #    "/content/drive/MyDrive/doc1.pdf",
    #    "/content/drive/MyDrive/doc2.pdf",
    #]
    #texts = extract_texts_from_multiple_pdfs(pdf_files, split_mode='smart')



    print(f"ğŸ“š ì´ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

    # ë¼ë²¨ë§ ì‹¤í–‰
    checkpoint_path = os.path.join(OUTPUT_DIR, "checkpoint.csv")
    df_results = labeler.label_texts(
        texts=texts,
        batch_save=10,
        checkpoint_path=checkpoint_path
    )

    # ê²°ê³¼ ìš”ì•½
    labeler.print_summary()

    # ê²°ê³¼ ì €ì¥
    csv_path, excel_path, json_path = labeler.save_results(OUTPUT_DIR)

    # ì‹œê°í™”
    graph_path = os.path.join(OUTPUT_DIR, "difficulty_distribution.png")
    labeler.visualize_results(save_path=graph_path)

    print("\n" + "="*50)
    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*50)
    print(f"ì €ì¥ëœ íŒŒì¼:")
    print(f"  ğŸ“„ CSV: {csv_path}")
    print(f"  ğŸ“Š Excel: {excel_path}")
    print(f"  ğŸ“‹ JSON: {json_path}")
    print(f"  ğŸ“ˆ ê·¸ë˜í”„: {graph_path}")

    return df_results


# ì‹¤í–‰
if __name__ == "__main__":
    results = main()